# ðŸ§  From GCNs to GraphSAGE â€” Graph Neural Networks from Scratch in PyTorch

This repository is part of my **PyTorch Research Mastery Series**, where I implement deep learning architectures **from scratch**, without relying on high-level abstractions.  

This particular project focuses on **Graph Neural Networks (GNNs)** â€” starting with a hand-built **Graph Convolutional Network (GCN)**, implemented purely using PyTorch tensor operations and adjacency normalization math.  
The repository is fully modular and structured to later include **Graph Attention Networks (GAT)** and **GraphSAGE**, extending this framework into attention-based and inductive GNN paradigms.

---

## ðŸ§± Research Intent

This repository is part of my ongoing **PyTorch Deep Research Grind** â€” a series of self-driven, low-level implementations aimed at mastering AI architectures from first principles.  
Rather than using convenience wrappers, each project reconstructs model internals using **raw tensor operations, autograd, and linear algebra**.

> **Objective:** Build an unshakable foundation in the theory and implementation of GNNs before transitioning to more advanced forms (GAT, GraphSAGE, and Transformers on Graphs).

---

## ðŸ§© Overview

Graph Neural Networks (GNNs) are a fundamental class of deep learning models designed to operate on **non-Euclidean data** â€” graphs, networks, molecules, and relational systems.  

This repo demonstrates:
- **A GCN implemented completely from scratch** (no `torch_geometric.nn`).
- Flexible **dataset support** â€” any graph dataset (Cora, PubMed, custom, etc.).
- Modular design for seamless expansion into **GAT** and **GraphSAGE**.
- Reproducible, research-ready codebase structure following FAIR-style conventions.

---

## âš™ï¸ Key Features

| Component | File | Description |
|------------|------|-------------|
| ðŸ§± `GCNFromScratch` | `models/gcn.py` | Pure PyTorch GCN using manual adjacency normalization and graph convolution |
| âš™ï¸ `data_loader.py` | | Loads built-in or user-defined graph datasets |
| ðŸ”§ `config.py` | | Centralized configuration for hyperparameters |
| ðŸ§© `gat.py` | | Placeholder for Graph Attention Network (upcoming extension) |
| ðŸ§© `graphsage.py` | | Placeholder for GraphSAGE inductive learning (upcoming extension) |

---

## ðŸ§± Architecture

The **Graph Convolutional Network (GCN)** implemented here follows the mathematical formulation introduced by Kipf & Welling (2016):

```
H^(l+1) = Ïƒ(DÌ‚^(-1/2) Ã‚ DÌ‚^(-1/2) H^(l) W^(l))
```

where:
- `Ã‚ = A + I`: adjacency matrix with self-loops  
- `DÌ‚`: degree matrix of `Ã‚`  
- `W^(l)`: learnable weight matrix  
- `Ïƒ`: non-linearity (ReLU)  

The model stacks two such layers, with dropout and softmax normalization for classification.

---

## ðŸ§® Implementation Details

The implementation explicitly constructs the adjacency matrix, normalizes it using `D^(-1/2) A D^(-1/2)`, and performs propagation via matrix multiplications.  
This exercise was done intentionally to internalize the underlying linear algebra and propagation mechanisms behind GCNs â€” instead of relying on prebuilt PyG modules.

---

## ðŸ“¦ Dataset Support

This framework supports **any dataset** compatible with `torch_geometric.data.Data`.  

### âœ… Built-in Datasets (PyG)
- Cora  
- Citeseer  
- PubMed  

### âœ… Custom Graphs
You can create your own dataset directly:
```python
from torch_geometric.data import Data
from data_loader import create_custom_graph
import torch

x = torch.randn(10, 4)  # 10 nodes, 4 features each
edge_index = torch.tensor([[0, 1, 2], [1, 2, 3]])  # edges (source -> target)
y = torch.randint(0, 2, (10,))  # binary node labels

data = create_custom_graph(x, edge_index, y=y)
```

---

## ðŸ§  Training

Run the training script with any dataset you specify in `config.py`:

```bash
python main.py
```

You can adjust the dataset and hyperparameters in `config.py`:

```python
class Config:
    dataset_name = "Cora"   # or "PubMed", "Citeseer", or a custom graph
    hidden_channels = 16
    learning_rate = 0.01
    weight_decay = 5e-4
    num_epochs = 200
```

---

## ðŸ“Š Output Example

During training:

```
Epoch 010 | Loss: 0.9821 | Train: 0.8234 | Val: 0.8070 | Test: 0.7961
Epoch 020 | Loss: 0.7195 | Train: 0.8723 | Val: 0.8341 | Test: 0.8117
...
```

---

## ðŸ§­ Roadmap

| Stage | Model | Status |
|-------|-------|--------|
| 1ï¸âƒ£ | **GCN (from scratch)** | âœ… Completed |
| 2ï¸âƒ£ | **GAT (Graph Attention Network)** | â³ In progress â€” to be integrated next |
| 3ï¸âƒ£ | **GraphSAGE (Inductive Aggregation)** | ðŸ”œ Planned |
| 4ï¸âƒ£ | **Transformer-based Graph Networks** | ðŸš§ Future research goal |

---

## ðŸ§© Folder Structure

```
gcn-from-scratch/
â”‚
â”œâ”€â”€ main.py
â”œâ”€â”€ config.py
â”œâ”€â”€ data_loader.py
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ gcn.py
â”‚   â”œâ”€â”€ gat.py
â”‚   â””â”€â”€ graphsage.py
â”‚
â””â”€â”€ README.md
```

---

## ðŸ§° Requirements

```
torch
torch-geometric
torch-sparse
torch-scatter
numpy
```

---

## ðŸ“œ Citation

If you use or reference this work:

```
Arnav Mishra, "From GCNs to GraphSAGE â€” Graph Neural Networks from Scratch in PyTorch", 2025.
```

---

## ðŸ Author

**Arnav Mishra**  
AI Researcher Â· PyTorch Core & Graph Neural Networks Enthusiast  
Bhopal, India

---

> *"Understanding GNNs begins where you stop importing prebuilt layers."*
